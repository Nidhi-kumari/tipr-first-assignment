{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nidhi/anaconda3/lib/python3.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import LSHForest\n",
    "import math\n",
    "import operator\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "X_data = pd.read_csv(\"../data/dolphins/dolphins.csv\",encoding = 'utf8',sep='\\s+',header=None)\n",
    "y_data = pd.read_csv(\"../data/dolphins/dolphins_label.csv\",encoding = 'utf8',sep='\\s+',names=['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nidhi/anaconda3/lib/python3.7/site-packages/sklearn/neighbors/approximate.py:220: DeprecationWarning: LSHForest has poor performance and has been deprecated in 0.19. It will be removed in version 0.21.\n",
      "  DeprecationWarning)\n",
      "/home/nidhi/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/nidhi/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [0]]\n",
      "-----------\n",
      "[0, 0, 0, 0, 0, 2, 0]\n",
      "[[1]\n",
      " [3]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "-----------\n",
      "[1, 0, 1, 1, 1, 1, 1]\n",
      "[[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [2]]\n",
      "-----------\n",
      "[1, 1, 0, 0, 0, 0]\n",
      "[[2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]]\n",
      "-----------\n",
      "[2, 2, 2, 2, 1, 1]\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "-----------\n",
      "[1, 1, 1, 1, 1, 0]\n",
      "[[0]\n",
      " [2]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [2]]\n",
      "-----------\n",
      "[0, 2, 0, 1, 2, 2]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "-----------\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[[0]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]]\n",
      "-----------\n",
      "[0, 0, 2, 2, 2, 1]\n",
      "[[1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]]\n",
      "-----------\n",
      "[1, 1, 2, 2, 2, 1]\n",
      "[[0]\n",
      " [3]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "-----------\n",
      "[0, 1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X_data)\n",
    "y = np.array(y_data)\n",
    "kf = KFold(y.shape[0],n_folds=10)\n",
    "lshf = LSHForest(random_state=42)\n",
    "n_neighbors=5\n",
    "\n",
    "\n",
    "\n",
    "for train_index, validation_index in kf:\n",
    "            \n",
    "    X_train, X_val = X[train_index], X[validation_index]\n",
    "    y_train, y_val = y[train_index], y[validation_index]\n",
    "    #print(y_train[0])\n",
    "    lshf.fit(X_train)\n",
    "    distances, indices = lshf.kneighbors(X_val, n_neighbors=5)\n",
    "    index = indices.tolist()\n",
    "    predicted = []\n",
    "    predicted_label=[]\n",
    "    for i in index:\n",
    "        row=[]\n",
    "        for j in i:\n",
    "            \n",
    "            y_pred = y_train[j]\n",
    "            row.append(y_pred)\n",
    "        \n",
    "        predicted.append(row)\n",
    "    #print(np.asarray(predicted))\n",
    "    final=np.resize(np.asarray(predicted),(X_val.shape[0],n_neighbors))\n",
    "    for lst in final.tolist():\n",
    "        y_val_pred = most_common(lst)\n",
    "        predicted_label.append(y_val_pred)\n",
    "    print(y_val) \n",
    "    y_test=np.resize(y_val,(1,X_val.shape[0])).tolist()[0]\n",
    "    #print(y_test.tolist()[0])\n",
    "    print(\"-----------\")\n",
    "    print(predicted_label)\n",
    "    accuracy = accuracy_score(y_test, predicted_label)\n",
    "    f1scoremacro = f1_score(y_test, predicted_label,average='macro')\n",
    "    f1scoremicro = f1_score(y_test, predicted_label,average='micro')\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed=pd.read_csv(\"../data/pubmed/pubmed.csv\",encoding = 'utf8',sep='\\s+',header=None)\n",
    "from sklearn.metrics import f1_score\n",
    "def evaluation_pubmedLSH():\n",
    "    Accuracy=dict()\n",
    "    F1scoremacro=dict()\n",
    "    F1scoremicro=dict()\n",
    "    for file in [2,4,8,16,32]:\n",
    "        filename =  '../Reduced_dim/pubmed/pubmed_%d.csv'%(file,)\n",
    "        print(filename)\n",
    "        X=pd.read_csv(filename,encoding = 'utf8',header=None)\n",
    "        y=pd.read_csv(\"../data/pubmed/pubmed_label.csv\",encoding = 'utf8',names = [\"label\"])\n",
    "        \n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "    \n",
    "        kf = KFold(y.shape[0],n_folds=10)\n",
    "        lshf = LSHForest(random_state=42)\n",
    "        n_neighbors=5\n",
    "        \n",
    "        \n",
    "        accuracies=[]\n",
    "        f1scoremacro=[]\n",
    "        f1scoremicro=[]\n",
    "        for train_index, validation_index in kf:\n",
    "           #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            \n",
    "            #print(X)\n",
    "            \n",
    "            X_train, X_val = X[train_index], X[validation_index]\n",
    "            y_train, y_val = y[train_index], y[validation_index]\n",
    "            lshf.fit(X_train)\n",
    "            distances, indices = lshf.kneighbors(X_val, n_neighbors=5)\n",
    "            index = indices.tolist()\n",
    "            predicted = []\n",
    "            predicted_label=[]\n",
    "            for i in index:\n",
    "                row=[]\n",
    "                for j in i:\n",
    "\n",
    "                    y_pred = y_train[j]\n",
    "                    row.append(y_pred)\n",
    "\n",
    "                predicted.append(row)\n",
    "            #print(np.asarray(predicted))\n",
    "            final=np.resize(np.asarray(predicted),(X_val.shape[0],n_neighbors))\n",
    "            for lst in final.tolist():\n",
    "                y_val_pred = most_common(lst)\n",
    "                predicted_label.append(y_val_pred)\n",
    "            #print(y_val) \n",
    "            y_test=np.resize(y_val,(1,X_val.shape[0])).tolist()[0]\n",
    "            #print(y_test.tolist()[0])\n",
    "            #print(\"-----------\")\n",
    "            #print(predicted_label)\n",
    "            accuracy = accuracy_score(y_test, predicted_label)\n",
    "            f1_scoremacro = f1_score(y_test, predicted_label,average='macro')\n",
    "            f1_scoremicro = f1_score(y_test, predicted_label,average='micro')\n",
    "            accuracies.append(accuracy)\n",
    "            f1scoremacro.append(f1_scoremacro)\n",
    "            f1scoremicro.append(f1_scoremicro)\n",
    "        #print(accuracies)\n",
    "        #print(sum(accuracies)/float(len(accuracies)))\n",
    "        Accuracy[file]=sum(accuracies)/float(len(accuracies))\n",
    "        F1scoremacro[file]=sum(f1scoremacro)/float(len(f1scoremacro))\n",
    "        F1scoremicro[file]=sum(f1scoremicro)/float(len(f1scoremicro))\n",
    "    return Accuracy,F1scoremacro,F1scoremicro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Reduced_dim/pubmed/pubmed_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nidhi/anaconda3/lib/python3.7/site-packages/sklearn/neighbors/approximate.py:220: DeprecationWarning: LSHForest has poor performance and has been deprecated in 0.19. It will be removed in version 0.21.\n",
      "  DeprecationWarning)\n",
      "/home/nidhi/anaconda3/lib/python3.7/site-packages/sklearn/random_projection.py:378: DataDimensionalityWarning: The number of components is higher than the number of features: n_features < n_components (2 < 32).The dimensionality of the problem will not be reduced.\n",
      "  DataDimensionalityWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Reduced_dim/pubmed/pubmed_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nidhi/anaconda3/lib/python3.7/site-packages/sklearn/neighbors/approximate.py:220: DeprecationWarning: LSHForest has poor performance and has been deprecated in 0.19. It will be removed in version 0.21.\n",
      "  DeprecationWarning)\n",
      "/home/nidhi/anaconda3/lib/python3.7/site-packages/sklearn/random_projection.py:378: DataDimensionalityWarning: The number of components is higher than the number of features: n_features < n_components (4 < 32).The dimensionality of the problem will not be reduced.\n",
      "  DataDimensionalityWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Reduced_dim/pubmed/pubmed_8.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nidhi/anaconda3/lib/python3.7/site-packages/sklearn/neighbors/approximate.py:220: DeprecationWarning: LSHForest has poor performance and has been deprecated in 0.19. It will be removed in version 0.21.\n",
      "  DeprecationWarning)\n",
      "/home/nidhi/anaconda3/lib/python3.7/site-packages/sklearn/random_projection.py:378: DataDimensionalityWarning: The number of components is higher than the number of features: n_features < n_components (8 < 32).The dimensionality of the problem will not be reduced.\n",
      "  DataDimensionalityWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Reduced_dim/pubmed/pubmed_16.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nidhi/anaconda3/lib/python3.7/site-packages/sklearn/neighbors/approximate.py:220: DeprecationWarning: LSHForest has poor performance and has been deprecated in 0.19. It will be removed in version 0.21.\n",
      "  DeprecationWarning)\n",
      "/home/nidhi/anaconda3/lib/python3.7/site-packages/sklearn/random_projection.py:378: DataDimensionalityWarning: The number of components is higher than the number of features: n_features < n_components (16 < 32).The dimensionality of the problem will not be reduced.\n",
      "  DataDimensionalityWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Reduced_dim/pubmed/pubmed_32.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nidhi/anaconda3/lib/python3.7/site-packages/sklearn/neighbors/approximate.py:220: DeprecationWarning: LSHForest has poor performance and has been deprecated in 0.19. It will be removed in version 0.21.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "Accuracy,F1scoremacro,F1scoremicro = evaluation_pubmedLSH()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 0.3544743956687358, 4: 0.3520604296274525, 8: 0.3598380441197396, 16: 0.3644754920141732, 32: 0.363461139219542}\n"
     ]
    }
   ],
   "source": [
    "print(Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "f = open(\"evaluation_pubmedLSH\", \"wb\")\n",
    "pickle.dump(Accuracy,f)\n",
    "pickle.dump(F1scoremacro, f)\n",
    "pickle.dump(F1scoremicro,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"evaluation_pubmedLSH\", \"rb\")\n",
    "value1 = pickle.load(f)\n",
    "value2 = pickle.load(f)\n",
    "value3 = pickle.load(f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 0.3544743956687358, 4: 0.3520604296274525, 8: 0.3598380441197396, 16: 0.3644754920141732, 32: 0.363461139219542}\n",
      "{2: 0.33043870612100046, 4: 0.3316484202782873, 8: 0.33899516150998166, 16: 0.3446773648411173, 32: 0.3425210504477797}\n",
      "{2: 0.3544743956687358, 4: 0.3520604296274525, 8: 0.3598380441197396, 16: 0.3644754920141732, 32: 0.363461139219542}\n"
     ]
    }
   ],
   "source": [
    "print(value1)\n",
    "print(value2)\n",
    "print(value3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score\n",
    "def evaluation_twitterLSH():\n",
    "    Accuracy=dict()\n",
    "    F1scoremacro=dict()\n",
    "    F1scoremicro=dict()\n",
    "    \n",
    "    \n",
    "    file_y = \"../data/twitter/twitter_label.txt\"\n",
    "    with open(file_y) as f:\n",
    "        content_y = f.readlines()\n",
    "    y = [str(x.strip()) for x in content_y]\n",
    "    y = np.array(y)\n",
    "    \n",
    "       \n",
    "    \n",
    "    for file in [2,4,8,16,32,64,128,256,512,1024]:\n",
    "        filename =  '../Reduced_dim/twitter/twitter_%d.csv'%(file,)\n",
    "        print(filename)\n",
    "        X=pd.read_csv(filename,encoding = 'utf8',header=None)\n",
    "        \n",
    "    \n",
    "        X = np.array(X)\n",
    "        \n",
    "    \n",
    "        kf = KFold(y.shape[0],n_folds=10)\n",
    "        lshf = LSHForest(random_state=42)\n",
    "        n_neighbors=5\n",
    "        \n",
    "        \n",
    "        accuracies=[]\n",
    "        f1scoremacro=[]\n",
    "        f1scoremicro=[]\n",
    "        for train_index, validation_index in kf:\n",
    "           #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            \n",
    "            #print(X)\n",
    "            \n",
    "            X_train, X_val = X[train_index], X[validation_index]\n",
    "            y_train, y_val = y[train_index], y[validation_index]\n",
    "            lshf.fit(X_train)\n",
    "            distances, indices = lshf.kneighbors(X_val, n_neighbors=5)\n",
    "            index = indices.tolist()\n",
    "            predicted = []\n",
    "            predicted_label=[]\n",
    "            for i in index:\n",
    "                row=[]\n",
    "                for j in i:\n",
    "\n",
    "                    y_pred = y_train[j]\n",
    "                    row.append(y_pred)\n",
    "\n",
    "                predicted.append(row)\n",
    "            #print(np.asarray(predicted))\n",
    "            final=np.resize(np.asarray(predicted),(X_val.shape[0],n_neighbors))\n",
    "            for lst in final.tolist():\n",
    "                y_val_pred = most_common(lst)\n",
    "                predicted_label.append(y_val_pred)\n",
    "            #print(y_val) \n",
    "            y_test=np.resize(y_val,(1,X_val.shape[0])).tolist()[0]\n",
    "            #print(y_test.tolist()[0])\n",
    "            #print(\"-----------\")\n",
    "            #print(predicted_label)\n",
    "            accuracy = accuracy_score(y_test, predicted_label)\n",
    "            f1_scoremacro = f1_score(y_test, predicted_label,average='macro')\n",
    "            f1_scoremicro = f1_score(y_test, predicted_label,average='micro')\n",
    "            accuracies.append(accuracy)\n",
    "            f1scoremacro.append(f1_scoremacro)\n",
    "            f1scoremicro.append(f1_scoremicro)\n",
    "        #print(accuracies)\n",
    "        #print(sum(accuracies)/float(len(accuracies)))\n",
    "        Accuracy[file]=sum(accuracies)/float(len(accuracies))\n",
    "        F1scoremacro[file]=sum(f1scoremacro)/float(len(f1scoremacro))\n",
    "        F1scoremicro[file]=sum(f1scoremicro)/float(len(f1scoremicro))\n",
    "    return Accuracy,F1scoremacro,F1scoremicro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Reduced_dim/twitter/twitter_2.csv\n",
      "../Reduced_dim/twitter/twitter_4.csv\n",
      "../Reduced_dim/twitter/twitter_8.csv\n",
      "../Reduced_dim/twitter/twitter_16.csv\n",
      "../Reduced_dim/twitter/twitter_32.csv\n",
      "../Reduced_dim/twitter/twitter_64.csv\n",
      "../Reduced_dim/twitter/twitter_128.csv\n",
      "../Reduced_dim/twitter/twitter_256.csv\n",
      "../Reduced_dim/twitter/twitter_512.csv\n",
      "../Reduced_dim/twitter/twitter_1024.csv\n"
     ]
    }
   ],
   "source": [
    "Accuracy,F1scoremacro,F1scoremicro = evaluation_twitterLSH()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "f = open(\"evaluation_twitterLSH\", \"wb\")\n",
    "pickle.dump(Accuracy,f)\n",
    "pickle.dump(F1scoremacro, f)\n",
    "pickle.dump(F1scoremicro,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 0.4078333333333333, 4: 0.41100000000000003, 8: 0.41166666666666674, 16: 0.4141666666666667, 32: 0.44400000000000006, 64: 0.44300000000000006, 128: 0.45583333333333337, 256: 0.479, 512: 0.47833333333333333, 1024: 0.49849999999999994}\n"
     ]
    }
   ],
   "source": [
    "print(Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed=pd.read_csv(\"../data/dolphins/dolphins.csv\",encoding = 'utf8',sep='\\s+',header=None)\n",
    "from sklearn.metrics import f1_score\n",
    "def evaluation_dolphinsLSH():\n",
    "    Accuracy=dict()\n",
    "    F1scoremacro=dict()\n",
    "    F1scoremicro=dict()\n",
    "    for file in [2,4,8,16]:\n",
    "        filename =  '../Reduced_dim/dolphins/dolphins_%d.csv'%(file,)\n",
    "        print(filename)\n",
    "        X=pd.read_csv(filename,encoding = 'utf8',header=None)\n",
    "        y=pd.read_csv(\"../data/dolphins/dolphins_label.csv\",encoding = 'utf8',names = [\"label\"])\n",
    "        \n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "    \n",
    "        kf = KFold(y.shape[0],n_folds=10)\n",
    "        lshf = LSHForest(random_state=42)\n",
    "        n_neighbors=5\n",
    "        \n",
    "        \n",
    "        accuracies=[]\n",
    "        f1scoremacro=[]\n",
    "        f1scoremicro=[]\n",
    "        for train_index, validation_index in kf:\n",
    "           #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            \n",
    "            #print(X)\n",
    "            \n",
    "            X_train, X_val = X[train_index], X[validation_index]\n",
    "            y_train, y_val = y[train_index], y[validation_index]\n",
    "            lshf.fit(X_train)\n",
    "            distances, indices = lshf.kneighbors(X_val, n_neighbors=5)\n",
    "            index = indices.tolist()\n",
    "            predicted = []\n",
    "            predicted_label=[]\n",
    "            for i in index:\n",
    "                row=[]\n",
    "                for j in i:\n",
    "\n",
    "                    y_pred = y_train[j]\n",
    "                    row.append(y_pred)\n",
    "\n",
    "                predicted.append(row)\n",
    "            #print(np.asarray(predicted))\n",
    "            final=np.resize(np.asarray(predicted),(X_val.shape[0],n_neighbors))\n",
    "            for lst in final.tolist():\n",
    "                y_val_pred = most_common(lst)\n",
    "                predicted_label.append(y_val_pred)\n",
    "            #print(y_val) \n",
    "            y_test=np.resize(y_val,(1,X_val.shape[0])).tolist()[0]\n",
    "            #print(y_test.tolist()[0])\n",
    "            #print(\"-----------\")\n",
    "            #print(predicted_label)\n",
    "            accuracy = accuracy_score(y_test, predicted_label)\n",
    "            f1_scoremacro = f1_score(y_test, predicted_label,average='macro')\n",
    "            f1_scoremicro = f1_score(y_test, predicted_label,average='micro')\n",
    "            accuracies.append(accuracy)\n",
    "            f1scoremacro.append(f1_scoremacro)\n",
    "            f1scoremicro.append(f1_scoremicro)\n",
    "        #print(accuracies)\n",
    "        #print(sum(accuracies)/float(len(accuracies)))\n",
    "        Accuracy[file]=sum(accuracies)/float(len(accuracies))\n",
    "        F1scoremacro[file]=sum(f1scoremacro)/float(len(f1scoremacro))\n",
    "        F1scoremicro[file]=sum(f1scoremicro)/float(len(f1scoremicro))\n",
    "    return Accuracy,F1scoremacro,F1scoremicro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Reduced_dim/dolphins/dolphins_2.csv\n",
      "../Reduced_dim/dolphins/dolphins_4.csv\n",
      "../Reduced_dim/dolphins/dolphins_8.csv\n",
      "../Reduced_dim/dolphins/dolphins_16.csv\n"
     ]
    }
   ],
   "source": [
    "Accuracy,F1scoremacro,F1scoremicro = evaluation_dolphinsLSH()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "f = open(\"evaluation_dolphinsLSH\", \"wb\")\n",
    "pickle.dump(Accuracy,f)\n",
    "pickle.dump(F1scoremacro, f)\n",
    "pickle.dump(F1scoremicro,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 0.8023809523809524, 4: 0.8595238095238095, 8: 0.9047619047619049, 16: 0.9023809523809524}\n"
     ]
    }
   ],
   "source": [
    "print(Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score\n",
    "def evaluation_twitterLSHoriginal():\n",
    "    Accuracy=dict()\n",
    "    F1scoremacro=dict()\n",
    "    F1scoremicro=dict()\n",
    "    \n",
    "    \n",
    "    file_y = \"../data/twitter/twitter_label.txt\"\n",
    "    with open(file_y) as f:\n",
    "        content_y = f.readlines()\n",
    "    y = [str(x.strip()) for x in content_y]\n",
    "    y = np.array(y)\n",
    "    \n",
    "       \n",
    "    \n",
    "    for file in [2,4,8,16,32,64,128,256,512,1024]:\n",
    "        filename =  '../Reduced_dim/twitter/twitter_%d.csv'%(file,)\n",
    "        print(filename)\n",
    "        X=pd.read_csv(filename,encoding = 'utf8',header=None)\n",
    "        \n",
    "    \n",
    "        X = np.array(X)\n",
    "        \n",
    "    \n",
    "        kf = KFold(y.shape[0],n_folds=10)\n",
    "        lshf = LSHForest(random_state=42)\n",
    "        n_neighbors=5\n",
    "        \n",
    "        \n",
    "        accuracies=[]\n",
    "        f1scoremacro=[]\n",
    "        f1scoremicro=[]\n",
    "        for train_index, validation_index in kf:\n",
    "           #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            \n",
    "            #print(X)\n",
    "            \n",
    "            X_train, X_val = X[train_index], X[validation_index]\n",
    "            y_train, y_val = y[train_index], y[validation_index]\n",
    "            lshf.fit(X_train)\n",
    "            distances, indices = lshf.kneighbors(X_val, n_neighbors=5)\n",
    "            index = indices.tolist()\n",
    "            predicted = []\n",
    "            predicted_label=[]\n",
    "            for i in index:\n",
    "                row=[]\n",
    "                for j in i:\n",
    "\n",
    "                    y_pred = y_train[j]\n",
    "                    row.append(y_pred)\n",
    "\n",
    "                predicted.append(row)\n",
    "            #print(np.asarray(predicted))\n",
    "            final=np.resize(np.asarray(predicted),(X_val.shape[0],n_neighbors))\n",
    "            for lst in final.tolist():\n",
    "                y_val_pred = most_common(lst)\n",
    "                predicted_label.append(y_val_pred)\n",
    "            #print(y_val) \n",
    "            y_test=np.resize(y_val,(1,X_val.shape[0])).tolist()[0]\n",
    "            #print(y_test.tolist()[0])\n",
    "            #print(\"-----------\")\n",
    "            #print(predicted_label)\n",
    "            accuracy = accuracy_score(y_test, predicted_label)\n",
    "            f1_scoremacro = f1_score(y_test, predicted_label,average='macro')\n",
    "            f1_scoremicro = f1_score(y_test, predicted_label,average='micro')\n",
    "            accuracies.append(accuracy)\n",
    "            f1scoremacro.append(f1_scoremacro)\n",
    "            f1scoremicro.append(f1_scoremicro)\n",
    "        #print(accuracies)\n",
    "        #print(sum(accuracies)/float(len(accuracies)))\n",
    "        Accuracy[file]=sum(accuracies)/float(len(accuracies))\n",
    "        F1scoremacro[file]=sum(f1scoremacro)/float(len(f1scoremacro))\n",
    "        F1scoremicro[file]=sum(f1scoremicro)/float(len(f1scoremicro))\n",
    "    return Accuracy,F1scoremacro,F1scoremicro\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
