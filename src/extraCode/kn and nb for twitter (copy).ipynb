{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "file = '/Users/Vikas/Desktop/tipr-first-assignment-master/data/twitter/twitter.txt'\n",
    "with open(file) as f:\n",
    "    content = f.readlines()\n",
    "content = [str(x.strip()) for x in content] \n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(content)\n",
    "# summarize\n",
    "#print(vectorizer.vocabulary_)\n",
    "#print(vectorizer.idf_)\n",
    "# encode document\n",
    "vector = vectorizer.transform(content)\n",
    "# summarize encoded vector\n",
    "#print(vector.shape)\n",
    "X = vector.toarray()\n",
    "\n",
    "file_y = '/Users/Vikas/Desktop/tipr-first-assignment-master/data/twitter/twitter_label.txt'\n",
    "with open(file_y) as f:\n",
    "    content_y = f.readlines()\n",
    "y = [str(x.strip()) for x in content_y] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.340833333333\n",
      "0.33432140737\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1score = f1_score(y_test, y_pred,average='macro')\n",
    "print(accuracy)\n",
    "print(f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.519166666667\n",
      "0.447495032078\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X_train, y_train) \n",
    "y_predkn=neigh.predict(X_test)\n",
    "accuracykn = accuracy_score(y_test, y_predkn)\n",
    "f1scorekn = f1_score(y_test, y_predkn,average='macro')\n",
    "print(accuracykn)\n",
    "print(f1scorekn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def evaluation_twitterKNNLIB():\n",
    "    Accuracy=dict()\n",
    "    F1scoremacro=dict()\n",
    "    F1scoremicro=dict()\n",
    "    \n",
    "    file_y = \"../data/twitter/twitter_label.txt\"\n",
    "    with open(file_y) as f:\n",
    "        content_y = f.readlines()\n",
    "    y = [str(x.strip()) for x in content_y]\n",
    "    y = np.array(y)\n",
    "    \n",
    "    neigh = KNeighborsClassifier(n_neighbors=2)\n",
    "    for i in [2,4,8,16,32,64,128,256,512,1024]:\n",
    "        filename =  '../Reduced_dim/twitter/twitter_%d.csv'%(i,)\n",
    "        print(filename)\n",
    "        X=pd.read_csv(filename,encoding = 'utf8',header=None)\n",
    "        \n",
    "        \n",
    "        X = np.array(X)\n",
    "        \n",
    "    \n",
    "        kf = KFold(y.shape[0],n_folds=10)\n",
    "        \n",
    "        \n",
    "        accuracies=[]\n",
    "        f1scoremacro=[]\n",
    "        f1scoremicro=[]\n",
    "        for train_index, validation_index in kf:\n",
    "           #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            \n",
    "            #print(X)\n",
    "            \n",
    "            X_train, X_test = X[train_index], X[validation_index]\n",
    "            y_train, y_test = y[train_index], y[validation_index]\n",
    "            neigh.fit(X_train, y_train)\n",
    "            y_pred = neigh.predict(X_test)\n",
    "            \n",
    "            \n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            f1_scoremacro = f1_score(y_test, y_pred,average='macro')\n",
    "            f1_scoremicro = f1_score(y_test, y_pred,average='micro')\n",
    "            accuracies.append(accuracy)\n",
    "            f1scoremacro.append(f1_scoremacro)\n",
    "            f1scoremicro.append(f1_scoremicro)\n",
    "            \n",
    "            \n",
    "            \n",
    "        Accuracy[i]=sum(accuracies)/float(len(accuracies))\n",
    "        F1scoremacro[i]=sum(f1scoremacro)/float(len(f1scoremacro))\n",
    "        F1scoremicro[i]=sum(f1scoremicro)/float(len(f1scoremicro))\n",
    "    return Accuracy,F1scoremacro,F1scoremicro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Reduced_dim/twitter/twitter_2.csv\n",
      "../Reduced_dim/twitter/twitter_4.csv\n",
      "../Reduced_dim/twitter/twitter_8.csv\n",
      "../Reduced_dim/twitter/twitter_16.csv\n",
      "../Reduced_dim/twitter/twitter_32.csv\n",
      "../Reduced_dim/twitter/twitter_64.csv\n",
      "../Reduced_dim/twitter/twitter_128.csv\n",
      "../Reduced_dim/twitter/twitter_256.csv\n",
      "../Reduced_dim/twitter/twitter_512.csv\n",
      "../Reduced_dim/twitter/twitter_1024.csv\n"
     ]
    }
   ],
   "source": [
    "AccuracyKNNLIB,F1scoremacroKNNLIB,F1scoremicroKNNLIB = evaluation_twitterKNNLIB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "f = open(\"evaluation_twitterKNNLIB\", \"wb\")\n",
    "pickle.dump(AccuracyKNNLIB,f)\n",
    "pickle.dump(F1scoremacroKNNLIB, f)\n",
    "pickle.dump(F1scoremicroKNNLIB,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 0.332,\n",
       " 4: 0.3373333333333334,\n",
       " 8: 0.3368333333333333,\n",
       " 16: 0.3245,\n",
       " 32: 0.3458333333333333,\n",
       " 64: 0.36066666666666664,\n",
       " 128: 0.398,\n",
       " 256: 0.3923333333333333,\n",
       " 512: 0.3905,\n",
       " 1024: 0.4023333333333333}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AccuracyKNNLIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed=pd.read_csv(\"../data/pubmed/pubmed.csv\",encoding = 'utf8',sep='\\s+',header=None)\n",
    "from sklearn.metrics import f1_score\n",
    "def evaluation_twitterNBLIB():\n",
    "    Accuracy=dict()\n",
    "    F1scoremacro=dict()\n",
    "    F1scoremicro=dict()\n",
    "    clf = GaussianNB() \n",
    "    \n",
    "    file_y = \"../data/twitter/twitter_label.txt\"\n",
    "    with open(file_y) as f:\n",
    "        content_y = f.readlines()\n",
    "    y = [str(x.strip()) for x in content_y]\n",
    "    y = np.array(y)\n",
    "    \n",
    "    \n",
    "    for i in [2,4,8,16,32,64,128,256,512,1024]:\n",
    "        filename =  '../Reduced_dim/twitter/twitter_%d.csv'%(i,)\n",
    "        print(filename)\n",
    "        X=pd.read_csv(filename,encoding = 'utf8',header=None)\n",
    "        X = np.array(X)\n",
    "    \n",
    "        kf = KFold(y.shape[0],n_folds=10)\n",
    "        \n",
    "        \n",
    "        accuracies=[]\n",
    "        f1scoremacro=[]\n",
    "        f1scoremicro=[]\n",
    "        for train_index, validation_index in kf:\n",
    "           #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            \n",
    "            #print(X)\n",
    "            \n",
    "            X_train, X_test = X[train_index], X[validation_index]\n",
    "            y_train, y_test = y[train_index], y[validation_index]\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            \n",
    "            \n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            f1_scoremacro = f1_score(y_test, y_pred,average='macro')\n",
    "            f1_scoremicro = f1_score(y_test, y_pred,average='micro')\n",
    "            accuracies.append(accuracy)\n",
    "            f1scoremacro.append(f1_scoremacro)\n",
    "            f1scoremicro.append(f1_scoremicro)\n",
    "            \n",
    "            \n",
    "            \n",
    "        Accuracy[i]=sum(accuracies)/float(len(accuracies))\n",
    "        F1scoremacro[i]=sum(f1scoremacro)/float(len(f1scoremacro))\n",
    "        F1scoremicro[i]=sum(f1scoremicro)/float(len(f1scoremicro))\n",
    "    return Accuracy,F1scoremacro,F1scoremicro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Reduced_dim/twitter/twitter_2.csv\n",
      "../Reduced_dim/twitter/twitter_4.csv\n",
      "../Reduced_dim/twitter/twitter_8.csv\n",
      "../Reduced_dim/twitter/twitter_16.csv\n",
      "../Reduced_dim/twitter/twitter_32.csv\n",
      "../Reduced_dim/twitter/twitter_64.csv\n",
      "../Reduced_dim/twitter/twitter_128.csv\n",
      "../Reduced_dim/twitter/twitter_256.csv\n",
      "../Reduced_dim/twitter/twitter_512.csv\n",
      "../Reduced_dim/twitter/twitter_1024.csv\n"
     ]
    }
   ],
   "source": [
    "AccuracyNBLIB,F1scoremacroNBLIB,F1scoremicroNBLIB = evaluation_twitterNBLIB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "f = open(\"evaluation_twitterNBLIB\", \"wb\")\n",
    "pickle.dump(AccuracyNBLIB,f)\n",
    "pickle.dump(F1scoremacroNBLIB, f)\n",
    "pickle.dump(F1scoremicroNBLIB,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"evaluation_twitterNBLIB\", \"rb\")\n",
    "value1 = pickle.load(f)\n",
    "value2 = pickle.load(f)\n",
    "value3 = pickle.load(f)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 0.5156666666666666, 4: 0.5148333333333333, 8: 0.5113333333333332, 16: 0.4965, 32: 0.5031666666666667, 64: 0.4756666666666667, 128: 0.4736666666666666, 256: 0.4731666666666666, 512: 0.4798333333333333, 1024: 0.4986666666666667}\n"
     ]
    }
   ],
   "source": [
    "print(value1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
