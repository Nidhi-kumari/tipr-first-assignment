{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nidhi/anaconda3/lib/python3.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import accuracy_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separateByClass(dataset):\n",
    "    separated = {}\n",
    "    for index, row in dataset.iterrows():\n",
    "        vector = row\n",
    "        if (vector[-1] not in separated):\n",
    "            separated[vector[-1]] = []\n",
    "        separated[vector[-1]].append(list(vector))\n",
    "    return separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separateByClass(dataset):\n",
    "    separated = {}\n",
    "    for i in range(len(dataset)):\n",
    "        vector = dataset[i]\n",
    "        if (vector[-1] not in separated):\n",
    "            separated[vector[-1]] = []\n",
    "        separated[vector[-1]].append(vector)\n",
    "    return separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def mean(numbers):\n",
    "    return sum(numbers)/float(len(numbers))\n",
    " \n",
    "def stdev(numbers):\n",
    "    avg = mean(numbers)\n",
    "    #variance = sum([pow(x-avg,2) for x in numbers])/float(len(numbers)-1)\n",
    "    variance= np.var(numbers)\n",
    "    return float(math.sqrt(float(variance)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(dataset):\n",
    "    summaries = [(mean(attribute), stdev(attribute)) for attribute in zip(*dataset)]\n",
    "    del summaries[-1]\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def summarizeByClass(dataset):\n",
    "    separated = separateByClass(dataset)\n",
    "    summaries = {}\n",
    "    for classValue, instances in separated.items():\n",
    "        summaries[classValue] = summarize(instances)\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def calculateProbability(x, mean, stdev):\n",
    "    if stdev==0.0:\n",
    "        stdev=0.0000001\n",
    "    exponent = math.exp(-(math.pow(x-mean,2)/float((2*float(math.pow(stdev,2)))) ))\n",
    "    \n",
    "    return (1 / float((math.sqrt(2*math.pi) * float(stdev)))) * exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateClassProbabilities(summaries, inputVector):\n",
    "\tprobabilities = {}\n",
    "\tfor classValue, classSummaries in summaries.items():\n",
    "\t\tprobabilities[classValue] = 1\n",
    "\t\tfor i in range(len(classSummaries)):\n",
    "\t\t\tmean, stdev = classSummaries[i]\n",
    "\t\t\tx = inputVector[i]\n",
    "\t\t\tprobabilities[classValue] *= calculateProbability(x, mean, stdev)\n",
    "\treturn probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(summaries, inputVector):\n",
    "\tprobabilities = calculateClassProbabilities(summaries, inputVector)\n",
    "\tbestLabel, bestProb = None, -1\n",
    "\tfor classValue, probability in probabilities.items():\n",
    "\t\tif bestLabel is None or probability > bestProb:\n",
    "\t\t\tbestProb = probability\n",
    "\t\t\tbestLabel = classValue\n",
    "\treturn bestLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictions(summaries, testSet):\n",
    "\tpredictions = []\n",
    "\tfor i in range(len(testSet)):\n",
    "\t\tresult = predict(summaries, testSet[i])\n",
    "\t\tpredictions.append(result)\n",
    "\treturn predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "def main():\n",
    "    y_test=[]\n",
    "    X_train, X_test = train_test_split(dataset, test_size=0.20, random_state=42)\n",
    "    print(X_train[10])\n",
    "    for i in range(len(X_test)):\n",
    "        \n",
    "        y_test.append(X_test[i][-1])\n",
    "    summaries = summarizeByClass(X_train)\n",
    "    # test model\n",
    "    predictions = getPredictions(summaries, X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    f1score = f1_score(y_test, predictions,average='macro')\n",
    "    return accuracy,f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "X_data = pd.read_csv(\"../data/dolphins/dolphins.csv\",encoding = 'utf8',sep='\\s+',header=None)\n",
    "lis = X_data.values.tolist()\n",
    "se = set(tuple(x) for x in lis)\n",
    "print(len(se))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dolphins=pd.read_csv(\"../data/dolphins/dolphins.csv\",encoding = 'utf8',sep='\\s+',header=None)\n",
    "from sklearn.metrics import f1_score\n",
    "def evaluation_pubmed():\n",
    "    f1score=[]\n",
    "    dimension=[]\n",
    "    accuracies=[]\n",
    "    \n",
    "    for i in [2,4,8,16]:\n",
    "        y_test=[]\n",
    "        filename =  '../Reduced_dim/dolphins/dolphins_%d.csv'%(i,)\n",
    "        dimension.append(i)\n",
    "        print(filename)\n",
    "        X = pd.read_csv(filename,encoding = 'utf8',header=None)\n",
    "        y = pd.read_csv(\"../data/dolphins/dolphins_label.csv\",encoding = 'utf8',names = [\"label\"])\n",
    "        df = pd.concat([X, y],axis =1)\n",
    "        dataset=df.values.tolist()\n",
    "        X_train, X_test = train_test_split(dataset, test_size=0.20, random_state=42)\n",
    "        for i in range(len(X_test)):\n",
    "            y_test.append(X_test[i][-1])\n",
    "        summaries = summarizeByClass(X_train)\n",
    "        #print(len(X_test))\n",
    "        #print(len(y_test))\n",
    "        predictions = getPredictions(summaries, X_test)\n",
    "        #print(len(predictions))\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        f1_score1 = f1_score(y_test, predictions,average='macro')\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        f1score.append(f1_score1)\n",
    "    return dimension,accuracies,f1scoremacro,f1scoremicro\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Reduced_dim/dolphins/dolphins_4.csv\n",
      "           0         1         2         3  label\n",
      "0  -2.765655  0.483368 -2.211931 -2.145444      0\n",
      "1  -2.866814  0.411703 -1.975392 -1.900132      0\n",
      "2  -2.967458  0.348385 -2.739848 -2.742685      0\n",
      "3  -3.998124  0.021523 -2.145289 -2.775127      0\n",
      "4  -2.374729  0.701865 -2.637285 -2.445750      0\n",
      "5  -3.717031  0.770845 -2.758317 -2.216343      2\n",
      "6  -2.727580  0.371336 -2.515996 -2.526860      0\n",
      "7  -0.963014  1.667537 -2.787501 -0.711976      1\n",
      "8  -2.105322  1.355979 -2.200019 -1.168732      3\n",
      "9   0.236845  2.584049 -3.248256  0.427743      1\n",
      "10  0.195717  2.578874 -3.403492  0.022561      1\n",
      "11 -0.500937  1.765171 -2.718238 -0.515167      1\n",
      "12 -0.141386  2.241485 -2.845882  0.164761      1\n",
      "13 -0.462423  2.200393 -3.146871 -0.576192      1\n",
      "14 -0.560231  2.109356 -3.062416 -0.581440      1\n",
      "15 -1.637668  1.281389 -2.621935 -1.444149      0\n",
      "16 -3.142681  0.040290 -2.122939 -2.943997      0\n",
      "17 -3.615036 -0.120379 -1.947925 -2.827007      0\n",
      "18 -3.596057 -0.063292 -2.364946 -3.213976      0\n",
      "19 -3.834869  0.601463 -2.517073 -2.525615      2\n",
      "20 -3.641256  0.775041 -2.624106 -2.320132      2\n",
      "21 -3.668368  1.243957 -3.122571 -2.138035      2\n",
      "22 -4.398296  0.353398 -3.093288 -2.736359      2\n",
      "23 -4.712968  0.720774 -3.423056 -2.654249      2\n",
      "24  0.495672  3.265203 -3.719116  0.667663      1\n",
      "25  0.467858  3.315354 -3.807252  0.481196      1\n",
      "26  0.327380  2.849785 -3.239055  0.434648      1\n",
      "27  0.240049  3.000088 -3.375798  0.526798      1\n",
      "28 -0.250190  2.322182 -2.915118  0.162328      1\n",
      "29  0.271013  2.888329 -3.469547  0.239652      1\n",
      "..       ...       ...       ...       ...    ...\n",
      "32 -3.143553  0.196467 -1.996478 -2.178429      0\n",
      "33 -3.978639  0.742796 -2.903937 -2.448997      2\n",
      "34 -3.050296  0.705738 -2.296458 -2.281079      0\n",
      "35  0.506809  3.095255 -3.466416  0.632188      1\n",
      "36 -4.355045  0.485936 -3.019548 -2.830439      2\n",
      "37 -4.399580  0.354153 -3.003189 -2.785793      2\n",
      "38 -3.933121 -0.061993 -2.027117 -2.866228      0\n",
      "39 -3.761257 -0.366691 -1.988619 -2.858822      0\n",
      "40 -4.037753 -0.721404 -1.805837 -3.304463      0\n",
      "41 -3.528444 -0.242285 -1.688889 -2.595064      0\n",
      "42 -4.016709 -0.283721 -2.036071 -3.093991      0\n",
      "43 -2.861612  0.275746 -1.841245 -2.083709      0\n",
      "44 -4.033519  0.102493 -2.573689 -2.829580      0\n",
      "45 -3.610948  0.348958 -2.298600 -2.535025      0\n",
      "46 -4.546233  0.452536 -2.918889 -2.797949      2\n",
      "47 -4.642555  0.532186 -3.114617 -2.946495      2\n",
      "48 -4.586310  0.248248 -2.971365 -3.003547      2\n",
      "49 -0.025461  2.658035 -3.306349  0.105288      1\n",
      "50 -0.624625  2.304942 -3.440897 -0.645954      1\n",
      "51 -0.113240  2.727283 -3.576167 -0.007706      1\n",
      "52 -4.258527  0.551344 -3.065202 -2.646111      2\n",
      "53 -3.812845  0.740911 -2.804939 -2.193421      2\n",
      "54 -4.317562  0.465696 -3.056371 -2.868206      2\n",
      "55  0.303196  2.881753 -3.389193  0.388363      1\n",
      "56 -4.623770 -1.199061 -1.881398 -3.626292      0\n",
      "57 -1.105704  1.798804 -2.232923 -0.436983      3\n",
      "58 -3.357666 -0.042922 -1.740441 -2.547627      0\n",
      "59 -4.339376 -1.199425 -1.746263 -3.404547      0\n",
      "60 -3.976123 -0.159174 -2.364261 -3.555980      0\n",
      "61  0.310258  2.848177 -3.122996  0.377788      1\n",
      "\n",
      "[62 rows x 5 columns]\n",
      "[0.406206, 0.406206, 0.406206, 0.406206, 0.406206, 0.406206, 0.406206, 0.406206, 0.406206, 0.406206, 0.406206, 0.406206, 0.406206]\n",
      "[0.165329, 0.12521400000000002, 0.173868, 0.047679, 0.341211, 0.079688, 0.080364, -0.007636, 0.009415, 0.44705900000000004, 0.032761, 0.10136, 0.069004]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "continuous is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-77595b9bbee0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdimension\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1scoremacro\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1scoremicro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation_dolphins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-62-90a60dd863f9>\u001b[0m in \u001b[0;36mevaluation_dolphins\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m#print(len(predictions))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mf1_score1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mdiffering_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# No metrics support \"multiclass-multioutput\" format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: continuous is not supported"
     ]
    }
   ],
   "source": [
    "dimension,accuracies,f1scoremacro,f1scoremicro = evaluation_dolphins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pubmed=pd.read_csv(\"../data/pubmed/pubmed.csv\",encoding = 'utf8',sep='\\s+',header=None)\n",
    "from sklearn.metrics import f1_score\n",
    "def evaluation_pubmed():\n",
    "    f1scoremacro=dict()\n",
    "    f1scoremicro=dict()\n",
    "    #dimension=dict()\n",
    "    accuracies=dict()\n",
    "    \n",
    "    for file in [2,4,8,16,32]:\n",
    "        \n",
    "        y_test=[]\n",
    "        filename =  '../Reduced_dim/pubmed/pubmed_%d.csv'%(file,)\n",
    "        #dimension.append(i)\n",
    "        print(filename)\n",
    "        X = pd.read_csv(filename,encoding = 'utf8',header=None)\n",
    "        y = pd.read_csv(\"../data/pubmed/pubmed_label.csv\",encoding = 'utf8',names = [\"label\"])\n",
    "        df = pd.concat([X, y],axis =1)\n",
    "        dataset=df.values.tolist()\n",
    "        X_train, X_test = train_test_split(dataset, test_size=0.20, random_state=42)\n",
    "        for i in range(len(X_test)):\n",
    "            y_test.append(X_test[i][-1])\n",
    "        summaries = summarizeByClass(X_train)\n",
    "        #print(len(X_test))\n",
    "        #print(len(y_test))\n",
    "        predictions = getPredictions(summaries, X_test)\n",
    "        #print(len(predictions))\n",
    "        #print(predictions)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        f1_score1 = f1_score(y_test, predictions,average='macro')\n",
    "        f1_score2 = f1_score(y_test,predictions,average ='micro')\n",
    "        #print(i)\n",
    "        accuracies[file]=accuracy\n",
    "        \n",
    "        f1scoremacro[file]=f1_score1\n",
    "        f1scoremicro[file]=f1_score2\n",
    "    return accuracies,f1scoremacro,f1scoremicro\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Reduced_dim/pubmed/pubmed_2.csv\n",
      "../Reduced_dim/pubmed/pubmed_4.csv\n",
      "../Reduced_dim/pubmed/pubmed_8.csv\n",
      "../Reduced_dim/pubmed/pubmed_16.csv\n",
      "../Reduced_dim/pubmed/pubmed_32.csv\n"
     ]
    }
   ],
   "source": [
    "accuracies,f1scoremacro,f1scoremicro = evaluation_pubmed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ::  {2: 0.3465346534653465, 4: 0.38589712629799566, 8: 0.3851726636078242, 16: 0.39603960396039606, 32: 0.41535860903163485}\n",
      "\n",
      "F1_score macro :: {2: 0.3374184296140646, 4: 0.35237111753073114, 8: 0.3691101417858031, 16: 0.38428105367881643, 32: 0.40009959050817995}\n",
      "\n",
      "F1_score micro :: {2: 0.3465346534653465, 4: 0.3858971262979956, 8: 0.3851726636078242, 16: 0.39603960396039606, 32: 0.41535860903163485}\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy :: \",accuracies)\n",
    "print()\n",
    "print(\"F1_score macro ::\",f1scoremacro)\n",
    "print()\n",
    "print(\"F1_score micro ::\",f1scoremicro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#pubmedResult = \"pickle.dat\"\n",
    "\n",
    "\n",
    "f = open(\"evaluation_pubmedNBS\", \"wb\")\n",
    "pickle.dump(accuracies, f)\n",
    "pickle.dump(f1scoremacro,f)\n",
    "pickle.dump(f1scoremicro,f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"evaluation_pubmedNBS\", \"rb\")\n",
    "value1 = pickle.load(f)\n",
    "value2 = pickle.load(f)\n",
    "value3 = pickle.load(f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 0.3465346534653465,\n",
       " 4: 0.38589712629799566,\n",
       " 8: 0.3851726636078242,\n",
       " 16: 0.39603960396039606,\n",
       " 32: 0.41535860903163485}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pubmed=pd.read_csv(\"../data/pubmed/pubmed.csv\",encoding = 'utf8',sep='\\s+',header=None)\n",
    "from sklearn.metrics import f1_score\n",
    "def evaluation_pubmedpca():\n",
    "    f1scoremacro=[]\n",
    "    f1scoremicro=[]\n",
    "    dimension=[]\n",
    "    accuracies=[]\n",
    "    \n",
    "    for i in range(2,int(pubmed.shape[1]/2 +1),2):\n",
    "        y_test=[]\n",
    "        filename =  '../pca_dim/pubmed/pubmed_%d.csv'%(i,)\n",
    "        dimension.append(i)\n",
    "        print(filename)\n",
    "        X = pd.read_csv(filename,encoding = 'utf8',header=None)\n",
    "        y = pd.read_csv(\"../data/pubmed/pubmed_label.csv\",encoding = 'utf8',names = [\"label\"])\n",
    "        df = pd.concat([X, y],axis =1)\n",
    "        dataset=df.values.tolist()\n",
    "        X_train, X_test = train_test_split(dataset, test_size=0.20, random_state=42)\n",
    "        for i in range(len(X_test)):\n",
    "            y_test.append(X_test[i][-1])\n",
    "        summaries = summarizeByClass(X_train)\n",
    "        #print(len(X_test))\n",
    "        #print(len(y_test))\n",
    "        predictions = getPredictions(summaries, X_test)\n",
    "        #print(len(predictions))\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        f1_score1 = f1_score(y_test, predictions,average='macro')\n",
    "        f1_scoremicro = f1_score(y_test, predictions,average='micro')\n",
    "        accuracies.append(accuracy)\n",
    "        f1scoremacro.append(f1_score1)\n",
    "        f1scoremicro.append(f1_scoremicro)\n",
    "    return dimension,accuracies,f1scoremacro,f1scoremicro\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../pca_dim/pubmed/pubmed_2.csv\n",
      "../pca_dim/pubmed/pubmed_4.csv\n",
      "../pca_dim/pubmed/pubmed_6.csv\n",
      "../pca_dim/pubmed/pubmed_8.csv\n",
      "../pca_dim/pubmed/pubmed_10.csv\n",
      "../pca_dim/pubmed/pubmed_12.csv\n",
      "../pca_dim/pubmed/pubmed_14.csv\n",
      "../pca_dim/pubmed/pubmed_16.csv\n",
      "../pca_dim/pubmed/pubmed_18.csv\n",
      "../pca_dim/pubmed/pubmed_20.csv\n",
      "../pca_dim/pubmed/pubmed_22.csv\n",
      "../pca_dim/pubmed/pubmed_24.csv\n",
      "../pca_dim/pubmed/pubmed_26.csv\n",
      "../pca_dim/pubmed/pubmed_28.csv\n",
      "../pca_dim/pubmed/pubmed_30.csv\n",
      "../pca_dim/pubmed/pubmed_32.csv\n",
      "../pca_dim/pubmed/pubmed_34.csv\n",
      "../pca_dim/pubmed/pubmed_36.csv\n",
      "../pca_dim/pubmed/pubmed_38.csv\n",
      "../pca_dim/pubmed/pubmed_40.csv\n",
      "../pca_dim/pubmed/pubmed_42.csv\n",
      "../pca_dim/pubmed/pubmed_44.csv\n",
      "../pca_dim/pubmed/pubmed_46.csv\n",
      "../pca_dim/pubmed/pubmed_48.csv\n",
      "../pca_dim/pubmed/pubmed_50.csv\n",
      "../pca_dim/pubmed/pubmed_52.csv\n",
      "../pca_dim/pubmed/pubmed_54.csv\n",
      "../pca_dim/pubmed/pubmed_56.csv\n",
      "../pca_dim/pubmed/pubmed_58.csv\n",
      "../pca_dim/pubmed/pubmed_60.csv\n",
      "../pca_dim/pubmed/pubmed_62.csv\n",
      "../pca_dim/pubmed/pubmed_64.csv\n"
     ]
    }
   ],
   "source": [
    "dimension,accuracies,f1scoremacro,f1scoremicro = evaluation_pubmedpca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#pubmedResult = \"pickle.dat\"\n",
    "\n",
    "#dimension,accuracies,f1scoremacro,f1scoremicro\n",
    "f = open(\"pubmedResultNBPCA\", \"wb\")\n",
    "pickle.dump(dimension,f)\n",
    "pickle.dump(accuracies, f)\n",
    "pickle.dump(f1scoremacro,f)\n",
    "pickle.dump(f1scoremicro,f)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"pubmedResultNBPCA\", \"rb\")\n",
    "value1 = pickle.load(f)\n",
    "value2 = pickle.load(f)\n",
    "value3 = pickle.load(f)\n",
    "value4 = pickle.load(f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def evaluation_twitterpca():\n",
    "    f1scoremacro=[]\n",
    "    f1scoremicro=[]\n",
    "    dimension=[]\n",
    "    accuracies=[]\n",
    "    \n",
    "    for i in [2,4,8,16,32,64,128,256,512,1024]:\n",
    "        y_test=[]\n",
    "        filename =  '../pca_dim/twitter/twitter_%d.csv'%(i,)\n",
    "        dimension.append(i)\n",
    "        print(filename)\n",
    "        X = pd.read_csv(filename,encoding = 'utf8',header=None)\n",
    "        #y = pd.read_csv(\"../twitter/twitter_label.csv\",encoding = 'utf8',names = [\"label\"])\n",
    "        file_y = \"../data/twitter/twitter_label.txt\"\n",
    "        with open(file_y) as f:\n",
    "            content_y = f.readlines()\n",
    "        y = [int(x.strip()) for x in content_y]\n",
    "\n",
    "        df = pd.concat([X, pd.DataFrame(y)],axis =1)\n",
    "        dataset=df.values.tolist()\n",
    "        X_train, X_test = train_test_split(dataset, test_size=0.20, random_state=42)\n",
    "        for i in range(len(X_test)):\n",
    "            y_test.append(X_test[i][-1])\n",
    "        summaries = summarizeByClass(X_train)\n",
    "        #print(len(X_test))\n",
    "        #print(len(y_test))\n",
    "        predictions = getPredictions(summaries, X_test)\n",
    "        #print(len(predictions))\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        f1_score1 = f1_score(y_test, predictions,average='macro')\n",
    "        f1_scoremicro = f1_score(y_test, predictions,average='micro')\n",
    "        accuracies.append(accuracy)\n",
    "        f1scoremacro.append(f1_score1)\n",
    "        f1scoremicro.append(f1_scoremicro)\n",
    "    return dimension,accuracies,f1scoremacro,f1scoremicro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../pca_dim/twitter/twitter_2.csv\n",
      "../pca_dim/twitter/twitter_4.csv\n",
      "../pca_dim/twitter/twitter_8.csv\n",
      "../pca_dim/twitter/twitter_16.csv\n",
      "../pca_dim/twitter/twitter_32.csv\n",
      "../pca_dim/twitter/twitter_64.csv\n",
      "../pca_dim/twitter/twitter_128.csv\n",
      "../pca_dim/twitter/twitter_256.csv\n",
      "../pca_dim/twitter/twitter_512.csv\n",
      "../pca_dim/twitter/twitter_1024.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nidhi/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "dimension,accuracies,f1scoremacro,f1scoremicro=evaluation_twitterpca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#pubmedResult = \"pickle.dat\"\n",
    "\n",
    "#dimension,accuracies,f1scoremacro,f1scoremicro\n",
    "f = open(\"twitterResultNBPCA\", \"wb\")\n",
    "pickle.dump(dimension,f)\n",
    "pickle.dump(accuracies, f)\n",
    "pickle.dump(f1scoremacro,f)\n",
    "pickle.dump(f1scoremicro,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def evaluation_twitter():\n",
    "    f1scoremacro=dict()\n",
    "    f1scoremicro=dict()\n",
    "    \n",
    "    accuracies=dict()\n",
    "    \n",
    "    for file in [2,4,8,16,32,64,128,256,512,1024]:\n",
    "        y_test=[]\n",
    "        filename =  '../Reduced_dim/twitter/twitter_%d.csv'%(file,)\n",
    "        \n",
    "        print(filename)\n",
    "        X = pd.read_csv(filename,encoding = 'utf8',header=None)\n",
    "        #y = pd.read_csv(\"../twitter/twitter_label.csv\",encoding = 'utf8',names = [\"label\"])\n",
    "        file_y = \"../data/twitter/twitter_label.txt\"\n",
    "        with open(file_y) as f:\n",
    "            content_y = f.readlines()\n",
    "        y = [int(x.strip()) for x in content_y]\n",
    "\n",
    "        df = pd.concat([X, pd.DataFrame(y)],axis =1)\n",
    "        dataset=df.values.tolist()\n",
    "        X_train, X_test = train_test_split(dataset, test_size=0.20, random_state=42)\n",
    "        for i in range(len(X_test)):\n",
    "            y_test.append(X_test[i][-1])\n",
    "        summaries = summarizeByClass(X_train)\n",
    "        #print(len(X_test))\n",
    "        #print(len(y_test))\n",
    "        predictions = getPredictions(summaries, X_test)\n",
    "        #print(len(predictions))\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        f1_score1 = f1_score(y_test, predictions,average='macro')\n",
    "        f1_scoremicro = f1_score(y_test, predictions,average='micro')\n",
    "        accuracies[file]=accuracy\n",
    "        f1scoremacro[file]=f1_score1\n",
    "        f1scoremicro[file]=f1_scoremicro\n",
    "    return accuracies,f1scoremacro,f1scoremicro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Reduced_dim/twitter/twitter_2.csv\n",
      "../Reduced_dim/twitter/twitter_4.csv\n",
      "../Reduced_dim/twitter/twitter_8.csv\n",
      "../Reduced_dim/twitter/twitter_16.csv\n",
      "../Reduced_dim/twitter/twitter_32.csv\n",
      "../Reduced_dim/twitter/twitter_64.csv\n",
      "../Reduced_dim/twitter/twitter_128.csv\n",
      "../Reduced_dim/twitter/twitter_256.csv\n",
      "../Reduced_dim/twitter/twitter_512.csv\n",
      "../Reduced_dim/twitter/twitter_1024.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nidhi/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "accuracies,f1scoremacro,f1scoremicro=evaluation_twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ::  {2: 0.2708333333333333, 4: 0.38, 8: 0.3466666666666667, 16: 0.3925, 32: 0.3616666666666667, 64: 0.4058333333333333, 128: 0.43666666666666665, 256: 0.4825, 512: 0.4708333333333333, 1024: 0.355}\n",
      "\n",
      "F1_score macro :: {2: 0.24922661224015288, 4: 0.3444679392526555, 8: 0.32906474510790085, 16: 0.3629725658766861, 32: 0.3349435069496241, 64: 0.38330596797521843, 128: 0.4023610064153553, 256: 0.4484349371940936, 512: 0.4346700611037404, 1024: 0.17466174661746617}\n",
      "\n",
      "F1_score micro :: {2: 0.2708333333333333, 4: 0.38, 8: 0.3466666666666667, 16: 0.3925, 32: 0.3616666666666667, 64: 0.4058333333333333, 128: 0.43666666666666665, 256: 0.4825, 512: 0.4708333333333333, 1024: 0.35500000000000004}\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy :: \",accuracies)\n",
    "print()\n",
    "print(\"F1_score macro ::\",f1scoremacro)\n",
    "print()\n",
    "print(\"F1_score micro ::\",f1scoremicro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#pubmedResult = \"pickle.dat\"\n",
    "\n",
    "#dimension,accuracies,f1scoremacro,f1scoremicro\n",
    "f = open(\"evaluation_twitterNBS\", \"wb\")\n",
    "\n",
    "pickle.dump(accuracies, f)\n",
    "pickle.dump(f1scoremacro,f)\n",
    "pickle.dump(f1scoremicro,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
